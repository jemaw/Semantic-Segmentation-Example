{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os \n",
    "import tensorflow as tf\n",
    "from unet_model_tf import unet_inference\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "from skimage import io, transform\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = './images'\n",
    "MASK_FOLDER = './masks'\n",
    "IMAGE_SIZE = 128 # For this example we resize the images, usually we would want to train on image patches\n",
    "BATCH_SIZE = 6\n",
    "OUTPUT_DIR = './output'\n",
    "KERNEL_NUM = 12\n",
    "DOUBLE_CONV = True\n",
    "SEED = 42\n",
    "\n",
    "NORMALIZER_PARAMS = {\n",
    "    # Decay for the moving averages.\n",
    "    'decay': 0.9,\n",
    "    # epsilon to prevent 0s in variance.\n",
    "    'epsilon': 0.001,\n",
    "    # scale\n",
    "    'scale': True,\n",
    "    # center\n",
    "    'center': True,\n",
    "    # renorm:\n",
    "    'renorm': False,\n",
    "}\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = glob(os.path.join(IMAGE_FOLDER, '*.png'))\n",
    "ids = [os.path.splitext(os.path.basename(x))[0] for x in all_images]\n",
    "# show some ids\n",
    "ids[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image, mask):\n",
    "    \"\"\"Augments image and mask\n",
    "    usually we want more functions here,\n",
    "    for example: flips, zooms, rotations, color distortions\n",
    "    \"\"\"\n",
    "    if np.random.binomial(1, 0.5):\n",
    "        image = np.fliplr(image)\n",
    "        mask = np.fliplr(mask)\n",
    "    if np.random.binomial(1, 0.5):\n",
    "        image = np.flipud(image)\n",
    "        mask = np.flipud(mask)\n",
    "    if np.random.binomial(1, 0.5):\n",
    "        degree = np.random.randint(-180, 180)\n",
    "        image = transform.rotate(image, degree)\n",
    "        mask = transform.rotate(mask, degree)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "def get_data_generator(ids, image_folder, mask_folder, image_size, batch_size, is_training):\n",
    "    # function that returns data generator\n",
    "    def data_generator():\n",
    "        # the generator required by tensorflow just loops forever\n",
    "        for i in itertools.count(1):\n",
    "            idx = ids[i % len(ids)]\n",
    "            image_path = os.path.join(image_folder, idx + '.png')\n",
    "            mask_path = os.path.join(mask_folder, idx+ '_mask.png')\n",
    "            \n",
    "            # read image and mask\n",
    "            image = io.imread(image_path, as_gray=True) # read image as grayscale\n",
    "            image = transform.resize(image, (image_size,image_size)).astype(np.float)\n",
    "            mask = io.imread(mask_path, as_gray=True)\n",
    "            mask = transform.resize(mask, (image_size,image_size)).astype(np.float)\n",
    "\n",
    "            if is_training:\n",
    "                image, mask = augment(image, mask)\n",
    "                \n",
    "            image -= 0.5\n",
    "            # we binarize because of the resizing which interpolates\n",
    "            mask[mask > 0.5] = 1\n",
    "            mask[mask <= 0.5] = 0\n",
    "            mask = np.expand_dims(mask, -1)\n",
    "            image = np.expand_dims(image, -1)\n",
    "            yield image, mask\n",
    "    return data_generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(ids, random_state=SEED)\n",
    "print('Trainids length: ', len(train_ids))\n",
    "print('Eval length: ', len(val_ids))\n",
    "train_generator = get_data_generator(train_ids, IMAGE_FOLDER, MASK_FOLDER, IMAGE_SIZE, BATCH_SIZE, True)\n",
    "val_generator = get_data_generator(val_ids, IMAGE_FOLDER, MASK_FOLDER, IMAGE_SIZE, 3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric for evaluation\n",
    "# dice coefficient\n",
    "def dice_tf(prediction, label):\n",
    "    prediction = tf.layers.flatten(tf.round(prediction))\n",
    "    label = tf.layers.flatten(tf.round(label))\n",
    "    intersection = prediction*label\n",
    "    dices = 2*tf.reduce_sum(intersection, -1)/(tf.reduce_sum(prediction, -1) + tf.reduce_sum(label, -1))\n",
    "    return tf.reduce_mean(dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # Create tensorflow dataset\n",
    "    ds = tf.data.Dataset().from_generator(train_generator, \n",
    "                                          (tf.float32, tf.float32), \n",
    "                                          ((IMAGE_SIZE, IMAGE_SIZE, 1), (IMAGE_SIZE, IMAGE_SIZE, 1)))\n",
    "    ds_val = tf.data.Dataset().from_generator(val_generator, \n",
    "                                          (tf.float32, tf.float32), \n",
    "                                          ((IMAGE_SIZE, IMAGE_SIZE, 1), (IMAGE_SIZE, IMAGE_SIZE, 1)))\n",
    "\n",
    "    # global step counts number of batches fed through network\n",
    "    global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False) \n",
    "\n",
    "\n",
    "    # Load unet definition\n",
    "    image, label = ds.prefetch(100).batch(BATCH_SIZE).make_one_shot_iterator().get_next() \n",
    "    model, _ = unet_inference(image, kernel_num=KERNEL_NUM,\n",
    "                                      is_training=True, \n",
    "                                      reuse=None, style='default', normalizer_params=NORMALIZER_PARAMS,\n",
    "                                     double_conv=DOUBLE_CONV, normalizer_fn=tf.contrib.layers.batch_norm)\n",
    "    \n",
    "    # Load unet definition for validation\n",
    "    image_val, label_val = ds_val.prefetch(20).batch(BATCH_SIZE).make_one_shot_iterator().get_next() \n",
    "    model_val, _ = unet_inference(image_val, kernel_num=KERNEL_NUM,\n",
    "                                      is_training=False, \n",
    "                                      reuse=True, style='default', normalizer_params=NORMALIZER_PARAMS,\n",
    "                                     double_conv=DOUBLE_CONV, normalizer_fn=tf.contrib.layers.batch_norm)\n",
    "    \n",
    "    \n",
    "\n",
    "    logits = model['logits']\n",
    "    mask_pred = model['probs']\n",
    "    logits_val = model_val['logits']\n",
    "    mask_pred_val = model_val['probs']\n",
    "    \n",
    "    # define loss\n",
    "    loss_op = tf.losses.sigmoid_cross_entropy(logits=logits, multi_class_labels=label)\n",
    "    loss_op_val = tf.losses.sigmoid_cross_entropy(logits=logits_val, multi_class_labels=label_val)\n",
    "    \n",
    "    # metrics\n",
    "    dice = dice_tf(mask_pred, label)\n",
    "    dice_val =  dice_tf(mask_pred_val, label_val)\n",
    "    \n",
    "    # needed for batch norm, see tf documentation of batch norm\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss_op, global_step=global_step)\n",
    "        \n",
    "    # define some summaries\n",
    "    with tf.name_scope('train'):\n",
    "        tf.summary.image('prediction', mask_pred, 1)\n",
    "        tf.summary.image('label', label, 1)\n",
    "        tf.summary.image('image', image, 1)\n",
    "        tf.summary.scalar('loss', loss_op)\n",
    "        tf.summary.scalar('dice', dice)\n",
    "    \n",
    "    with tf.name_scope('val'):\n",
    "        tf.summary.image('prediction_val', mask_pred_val, 1)\n",
    "        tf.summary.image('label_val', label_val, 1)\n",
    "        tf.summary.image('image_val', image_val, 1)\n",
    "        tf.summary.scalar('loss_val', loss_op_val)\n",
    "        tf.summary.scalar('dice_val', dice_val)\n",
    "    \n",
    "    sv = tf.train.Supervisor(logdir=OUTPUT_DIR,\n",
    "                             global_step=global_step,\n",
    "                             save_model_secs=300,\n",
    "                             save_summaries_secs=20)\n",
    "    \n",
    "    with sv.managed_session() as sess:\n",
    "        for step in range(10000):\n",
    "            _, im = sess.run([train_op, mask_pred])\n",
    "            if sv.should_stop():\n",
    "                break\n",
    "                                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "use tensorboard to track training progess and visualize computation graph (expand unet block)\n",
    "\n",
    "```tensorboard --logdir=output```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
